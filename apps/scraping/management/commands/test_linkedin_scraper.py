from django.core.management.base import BaseCommand
from apps.scraping.scrapers.linkedin_scraper import LinkedInScraper
from apps.scraping.models import Job
import logging

logger = logging.getLogger(__name__)

class Command(BaseCommand):
    help = 'Test LinkedIn scraper functionality'

    def handle(self, *args, **options):
        self.stdout.write("=== –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ LinkedIn —Å–∫—Ä–∞–ø–µ—Ä–∞ ===")
        
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–∫—Ä–∞–ø–µ—Ä–∞
            scraper = LinkedInScraper()
            self.stdout.write("‚úì LinkedIn —Å–∫—Ä–∞–ø–µ—Ä —Å–æ–∑–¥–∞–Ω")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π
            self.stdout.write("üîç –ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π...")
            jobs = scraper.search_jobs(['python', 'django'], 'Estonia', 2)
            
            self.stdout.write(f"‚úì –ù–∞–π–¥–µ–Ω–æ {len(jobs)} –≤–∞–∫–∞–Ω—Å–∏–π")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞–∫–∞–Ω—Å–∏–π
            for i, job in enumerate(jobs[:5]):
                self.stdout.write(f"\n--- –í–∞–∫–∞–Ω—Å–∏—è {i+1} ---")
                self.stdout.write(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {job.get('title', 'N/A')}")
                self.stdout.write(f"–ö–æ–º–ø–∞–Ω–∏—è: {job.get('company', 'N/A')}")
                self.stdout.write(f"–õ–æ–∫–∞—Ü–∏—è: {job.get('location', 'N/A')}")
                self.stdout.write(f"URL: {job.get('url', 'N/A')}")
                if job.get('salary_min'):
                    self.stdout.write(f"–ó–∞—Ä–ø–ª–∞—Ç–∞: {job.get('salary_min')} - {job.get('salary_max', 'N/A')}")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            self.stdout.write("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
            jobs_created = 0
            
            for job_data in jobs:
                try:
                    job, created = Job.objects.get_or_create(
                        source_url=job_data.get('url', ''),
                        defaults={
                            'title': job_data.get('title', ''),
                            'company_name': job_data.get('company', ''),
                            'location': job_data.get('location', ''),
                            'description': job_data.get('description', ''),
                            'source_site': 'linkedin',
                            'salary_min': job_data.get('salary_min'),
                            'salary_max': job_data.get('salary_max'),
                            'is_active': True
                        }
                    )
                    if created:
                        jobs_created += 1
                        self.stdout.write(f"‚úì –°–æ–∑–¥–∞–Ω–∞: {job.title} –≤ {job.company_name}")
                    else:
                        self.stdout.write(f"‚Ä¢ –£–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {job.title}")
                except Exception as e:
                    self.stdout.write(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {str(e)}")
            
            self.stdout.write(f"\n‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω! –°–æ–∑–¥–∞–Ω–æ {jobs_created} –Ω–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π –∏–∑ {len(jobs)} –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö")
            
        except Exception as e:
            self.stdout.write(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {str(e)}")
            logger.error(f"LinkedIn scraper test error: {str(e)}") 