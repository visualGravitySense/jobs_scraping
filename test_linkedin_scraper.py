#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ LinkedIn —Å–∫—Ä–∞–ø–µ—Ä–∞
"""

import sys
import os
import json
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from apps.scraping.scrapers.linkedin_scraper import LinkedInScraper

def test_linkedin_scraper():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ LinkedIn —Å–∫—Ä–∞–ø–µ—Ä–∞"""
    print("=" * 80)
    print("üîç Testing LinkedIn scraper...")
    print("=" * 80)
    
    try:
        # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–∫—Ä–∞–ø–µ—Ä–∞
        scraper = LinkedInScraper()
        
        # –ò—â–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ —Å —Ç–µ–º–∏ –∂–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, —á—Ç–æ –∏ –≤ –∑–∞–¥–∞—á–µ
        jobs = scraper.search_jobs(
            keywords=["software engineer", "developer", "programmer"],
            location="Estonia",
            max_pages=1  # –ù–∞—á–Ω–µ–º —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        )
        
        print(f"–ù–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {len(jobs)}")
        
        if jobs:
            print(f"\n–ü–µ—Ä–≤—ã–µ {min(10, len(jobs))} –≤–∞–∫–∞–Ω—Å–∏–π:")
            for i, job in enumerate(jobs[:10], 1):
                print(f"{i}. {job.get('title', 'N/A')}")
                print(f"   –ö–æ–º–ø–∞–Ω–∏—è: {job.get('company', 'N/A')}")
                print(f"   –õ–æ–∫–∞—Ü–∏—è: {job.get('location', 'N/A')}")
                print(f"   URL: {job.get('url', 'N/A')}")
                print()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
            urls = [job.get('url') for job in jobs if job.get('url')]
            unique_urls = set(urls)
            duplicate_urls = len(urls) - len(unique_urls)
            
            print(f"–í—Å–µ–≥–æ URL: {len(urls)}")
            print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL: {len(unique_urls)}")
            print(f"–î—É–±–ª–∏–∫–∞—Ç–æ–≤ URL: {duplicate_urls}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É –∏ –∫–æ–º–ø–∞–Ω–∏–∏
            job_signatures = []
            for job in jobs:
                signature = f"{job.get('title', '')} - {job.get('company', '')}"
                job_signatures.append(signature)
            
            unique_signatures = set(job_signatures)
            duplicate_signatures = len(job_signatures) - len(unique_signatures)
            
            print(f"–î—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É+–∫–æ–º–ø–∞–Ω–∏—è: {duplicate_signatures}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            if duplicate_signatures > 0:
                print(f"\n–î—É–±–ª–∏–∫–∞—Ç—ã:")
                seen = set()
                for i, signature in enumerate(job_signatures):
                    if signature in seen:
                        print(f"  –î—É–±–ª–∏–∫–∞—Ç {i+1}: {signature}")
                    else:
                        seen.add(signature)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON —Ñ–∞–π–ª
            with open("linkedin_jobs_debug.json", "w", encoding="utf-8") as f:
                json.dump(jobs, f, indent=2, ensure_ascii=False)
            print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ linkedin_jobs_debug.json")
        
        return len(jobs)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å–∫—Ä–∞–ø–µ—Ä–∞: {e}")
        import traceback
        traceback.print_exc()
        return 0

if __name__ == "__main__":
    job_count = test_linkedin_scraper()
    print(f"\n‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {job_count}") 